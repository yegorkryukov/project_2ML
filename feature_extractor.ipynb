{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependencies\n",
    "import pymongo as pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addToDB(DB_NAME,COL_NAME,PATH,FILE):\n",
    "    '''\n",
    "    Imports a file into mongoDB\n",
    "    \n",
    "    Parameters:\n",
    "    --------\n",
    "    DB_NAME : Name of the database to connect to\n",
    "    COL_NAME: Name of the collection to create\n",
    "    PATH    : Path to folder with the file\n",
    "    FILE  : Filename\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Collection COL_NAME in DB_NAME database\n",
    "    '''\n",
    "    !mongoimport --db {DB_NAME} --collection {COL_NAME} --file {PATH+FILE} --batchSize 1\n",
    "    print(f'Collection {COL_NAME} in {DB_NAME} database created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extractor(html):\n",
    "    '''\n",
    "    Parse html using newspaper\n",
    "    \n",
    "    Parameters:\n",
    "    --------\n",
    "    html   : 'string'\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    result : dictionary\n",
    "    \n",
    "    '''\n",
    "    import newspaper\n",
    "    from bs4 import BeautifulSoup as bs\n",
    "    from datetime import datetime\n",
    "    \n",
    "    article = newspaper.Article('')\n",
    "    article.set_html(html)\n",
    "    article.build()\n",
    "    \n",
    "    # parse date manually if it wasn't found by newspaper\n",
    "    if not article.publish_date:\n",
    "        sp = bs(html, 'lxml')\n",
    "        # 'dek___3AQpw' class appears on 30% of msnbc websites\n",
    "        try:\n",
    "            publish_date = datetime.strptime(\\\n",
    "                               (sp.find('p', class_='dek___3AQpw').span.text),\\\n",
    "                               '%b.%d.%Y'\\\n",
    "                            )\n",
    "        except:\n",
    "            publish_date = ''\n",
    "    else:\n",
    "        publish_date = article.publish_date\n",
    "\n",
    "\n",
    "    return {\n",
    "            'date'    :publish_date,\n",
    "            'title'   :article.title,\n",
    "            'text'    :article.text,\n",
    "            'authors' :article.authors,\n",
    "            'keywords':article.keywords\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def docs_parser(htmlCol):\n",
    "    '''\n",
    "    Parse mongo docs, extract features and update the doc with the features\n",
    "    \n",
    "    Parameters:\n",
    "    --------\n",
    "    htmlCol : mongodb collection, has to have documents with 'html' key\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    updates all documents in the collection\n",
    "    '''\n",
    "\n",
    "    for doc in htmlCol.find():\n",
    "        print(f\"{doc['_id']}:\")\n",
    "            \n",
    "        if 'html' in doc:\n",
    "            # extract metadata from html\n",
    "            meta = feature_extractor(doc['html'])\n",
    "            \n",
    "            try:\n",
    "                if meta == doc['meta']:\n",
    "                    print(f\"has same meta\")\n",
    "            except:\n",
    "                # if there is newer meta data or meta key is not existing\n",
    "                htmlCol.update_one(\n",
    "                    {'_id':ObjectId(doc['_id'])},\n",
    "                    {'$set' : {\n",
    "                              'meta' : meta\n",
    "                              }\n",
    "                    }\n",
    "                )\n",
    "                print(f\"updated meta\")\n",
    "        else:\n",
    "            print(f\"does not have html\")\n",
    "        print('----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_doc(id):\n",
    "    '''\n",
    "    Finds a document by 'id' and prints contents to the console\n",
    "    \n",
    "    Parameters:\n",
    "    --------\n",
    "    id : mongodb document id\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Prints first 100 symbols of each document's key to console\n",
    "    '''\n",
    "    from bson.objectid import ObjectId\n",
    "    doc = htmlCol.find_one({'_id':ObjectId('5b243fbc897e82028b8ffe52')})\n",
    "    for k in doc:\n",
    "        print(f\"{k} : {str(doc[k])[:100]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some comands to keep dbs clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yegor3/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: DeprecationWarning: update is deprecated. Use replace_one, update_one or update_many instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'n': 1586, 'nModified': 1586, 'ok': 1.0, 'updatedExisting': True}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# deletes all 'meta' fields from all docs\n",
    "# htmlCol.update({}, {$unset: {meta:1}}, false, true); # mongo shell comand\n",
    "htmlCol.update({}, {'$unset': {'meta':1}}, multi=True) # pymongo way\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'url_1'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# leaves only unique documents by 'url' field\n",
    "\n",
    "htmlCol.create_index(\n",
    "    \"url\",\n",
    "    unique=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pymongo 'find' returns cursor that allows iterating through results\n",
    "# calling first object [0] allows accessing the dictionary with results\n",
    "# the ['html'] is the key in the dictionary\n",
    "html = htmlCol.find({'url':'http://www.msnbc.com/velshi-ruhle/watch/jeff-sessions-is-justifying-harsh-immigration-policy-with-the-bible-1256689731629'},\\\n",
    "            projection={'html':True, '_id':False})[0]['html']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.msnbc.com/\n",
      "http://www.msnbc.com/{{path.prefix}}/transcripts\n",
      "http://www.msnbc.com/home2?page=1\n",
      "http://www.msnbc.com/rachel-maddow-show/trump-jr-offers-the-wrong-response-concerns-the-gop-cult\n",
      "http://www.msnbc.com/explore\n",
      "http://www.msnbc.com/rachel-maddow-show/president-tries-brush-lying-about-infamous-trump-tower-meeting\n",
      "http://www.msnbc.com/#main-menu\n",
      "http://www.msnbc.com/{{path.prefix}}/transcripts#main-menu\n",
      "https://www.facebook.com/sharer/sharer.php?u=https://www.msnbc.com/the-last-word/watch/lawrence-trump-tries-to-steal-the-grief-of-fallen-soldiers-parents-1256235075972\n",
      "http://www.msnbc.com/rachel-maddow-show/the-good-question-team-trump-considers-ridiculous-and-ludicrous\n",
      "http://www.msnbc.com/guns\n",
      "https://twitter.com/intent/tweet?text=Trump%20calls%20IG%20report%20'horror%20show'%2C%20blames%20Dems%20for%20separations&via=msnbc&url=https://www.msnbc.com/morning-joe/watch/trump-calls-ig-report-horror-show-blames-dems-for-separations-1256477251672&original_referer=https://www.msnbc.com/morning-joe/watch/trump-calls-ig-report-horror-show-blames-dems-for-separations-1256477251672\n",
      "http://www.msnbc.com/rachel-maddow-show/defamation-lawsuit-grows-more-serious-trump\n",
      "http://www.msnbc.com/money-in-politics\n",
      "http://www.msnbc.com/economy\n",
      "https://www.facebook.com/sharer/sharer.php?u=https://www.msnbc.com/morning-joe/watch/mika-again-ivanka-trump-misses-the-mark-1256438339637\n",
      "http://www.msnbc.com/rachel-maddow-show/thursdays-campaign-round-61418\n",
      "https://twitter.com/intent/tweet?text=Bible%20used%20to%20justify%20separating%20families%20at%20border&via=msnbc&url=https://www.msnbc.com/velshi-ruhle/watch/jeff-sessions-is-justifying-harsh-immigration-policy-with-the-bible-1256689731629&original_referer=https://www.msnbc.com/velshi-ruhle/watch/jeff-sessions-is-justifying-harsh-immigration-policy-with-the-bible-1256689731629\n",
      "https://www.facebook.com/sharer/sharer.php?u=https://www.msnbc.com/rachel-maddow/watch/gop-keeps-pressure-on-doj-in-effort-to-sink-trump-russia-probe-1256230979997\n",
      "http://www.msnbc.com/election-2014\n"
     ]
    }
   ],
   "source": [
    "# find documents NOT containing a 'tag': regex expression\n",
    "import re\n",
    "tag = re.compile('dek___3AQpw.')\n",
    "docs = htmlCol.find({\"html\" : {'$not': tag}})\n",
    "for d in docs[:20]: print(d['url'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "??test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = newspaper.Article(url='http://www.msnbc.com/rachel-maddow-show/defamation-lawsuit-grows-more-serious-trump')\n",
    "test.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(test.publish_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Article' object has no attribute 'date'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-150-ef7a1b307c94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Article' object has no attribute 'date'"
     ]
    }
   ],
   "source": [
    "test.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(dict,\n",
       "            {'og': {'url': 'http://www.msnbc.com/rachel-maddow-show/defamation-lawsuit-grows-more-serious-trump',\n",
       "              'type': 'article',\n",
       "              'site_name': 'MSNBC',\n",
       "              'title': 'Defamation lawsuit grows more serious for Trump',\n",
       "              'description': \"As if Donald Trump's lawyers weren't already busy, Summer Zervos' civil suit creates a real threat -- which isn't going away.\",\n",
       "              'image': 'http://www.msnbc.com/sites/msnbc/files/styles/ratio--1_91-1--1200x630/public/06525535.jpg?itok=lrFK_ych'},\n",
       "             'twitter': {'card': 'summary_large_image',\n",
       "              'site': '@msnbc',\n",
       "              'creator': {'identifier': '@msnbc', 'id': 2836421},\n",
       "              'url': 'http://www.msnbc.com/rachel-maddow-show/defamation-lawsuit-grows-more-serious-trump',\n",
       "              'account_id': 2836421,\n",
       "              'domain': 'msnbc.com',\n",
       "              'title': 'Defamation lawsuit grows more serious for Trump',\n",
       "              'description': \"As if Donald Trump's lawyers weren't already busy, Summer Zervos' civil suit creates a real threat -- which isn't going away.\",\n",
       "              'image': {'identifier': 'http://www.msnbc.com/sites/msnbc/files/styles/ratio--3-2--512x342/public/06525535.jpg?itok=3OMxjHMa',\n",
       "               'width': 830,\n",
       "               'height': 553}},\n",
       "             'original-source': 'http://www.msnbc.com/rachel-maddow-show/defamation-lawsuit-grows-more-serious-trump',\n",
       "             'viewport': 'width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no',\n",
       "             'msapplication-config': '/browserconfig.xml',\n",
       "             'fb': {'app_id': 351594351545594, 'pages': 809948162383434},\n",
       "             'nv': {'published': 1,\n",
       "              'address': 'http://www.msnbc.com/rachel-maddow-show/defamation-lawsuit-grows-more-serious-trump',\n",
       "              'title': 'Defamation lawsuit grows more serious for Trump',\n",
       "              'tags': 'donald-trump,equality,scandals,women,rachel-maddow-show',\n",
       "              'date': '06/15/2018 09:20:15 EDT',\n",
       "              'contentId': 'http://www.msnbc.com/rachel-maddow-show/defamation-lawsuit-grows-more-serious-trump',\n",
       "              'author': 'steve-benen'},\n",
       "             'description': \"As if Donald Trump's lawyers weren't already busy, Summer Zervos' civil suit creates a real threat -- which isn't going away.\",\n",
       "             'robots': 'follow, index',\n",
       "             'article': {'author': 'https://www.facebook.com/steve.benen'},\n",
       "             'jenga-session-config': '{\"when_to_init\":\"after_dom_ready\"}',\n",
       "             'abstract': \"As if Donald Trump's lawyers weren't already busy, Summer Zervos' civil suit creates a real threat -- which isn't going away.\",\n",
       "             'DC.date.issued': '2018-06-15T09:20:15-04:00',\n",
       "             'news_keywords': 'Equality , Women, Donald Trump, Scandals'})"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.meta_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Production code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PyMongo to work with MongoDBs\n",
    "conn = 'mongodb://localhost:27017'\n",
    "client = pm.MongoClient(conn)\n",
    "\n",
    "# define db \n",
    "DB_NAME = 'scrape'\n",
    "db = client[DB_NAME]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NR_DailyB_Intercept.json\n",
      "2018-06-26T21:40:06.483-0400\tconnected to: localhost\n",
      "2018-06-26T21:40:09.466-0400\t[#########...............] scrape.left\t317MB/828MB (38.3%)\n",
      "2018-06-26T21:40:12.466-0400\t[###################.....] scrape.left\t664MB/828MB (80.2%)\n",
      "2018-06-26T21:40:13.946-0400\t[########################] scrape.left\t828MB/828MB (100.0%)\n",
      "2018-06-26T21:40:13.946-0400\timported 4768 documents\n",
      "Collection left in scrape database created\n",
      "bbc.json\n",
      "2018-06-26T21:40:14.123-0400\tconnected to: localhost\n",
      "2018-06-26T21:40:17.112-0400\t[#####...................] scrape.left\t358MB/1.66GB (21.1%)\n",
      "2018-06-26T21:40:20.113-0400\t[#########...............] scrape.left\t685MB/1.66GB (40.4%)\n",
      "2018-06-26T21:40:23.110-0400\t[##############..........] scrape.left\t1.01GB/1.66GB (61.0%)\n",
      "2018-06-26T21:40:26.107-0400\t[###################.....] scrape.left\t1.33GB/1.66GB (80.3%)\n",
      "2018-06-26T21:40:29.062-0400\t[########################] scrape.left\t1.66GB/1.66GB (100.0%)\n",
      "2018-06-26T21:40:29.062-0400\timported 7921 documents\n",
      "Collection left in scrape database created\n",
      "guardian.json\n",
      "2018-06-26T21:40:29.240-0400\tconnected to: localhost\n",
      "2018-06-26T21:40:32.227-0400\t[####....................] scrape.left\t356MB/1.70GB (20.4%)\n",
      "2018-06-26T21:40:35.227-0400\t[##########..............] scrape.left\t761MB/1.70GB (43.7%)\n",
      "2018-06-26T21:40:38.227-0400\t[################........] scrape.left\t1.15GB/1.70GB (67.6%)\n",
      "2018-06-26T21:40:41.231-0400\t[######################..] scrape.left\t1.59GB/1.70GB (93.5%)\n",
      "2018-06-26T21:40:42.174-0400\t[########################] scrape.left\t1.70GB/1.70GB (100.0%)\n",
      "2018-06-26T21:40:42.174-0400\timported 3464 documents\n",
      "Collection left in scrape database created\n",
      "msnbc.json\n",
      "2018-06-26T21:40:42.362-0400\tconnected to: localhost\n",
      "2018-06-26T21:40:45.348-0400\t[##################......] scrape.left\t375MB/485MB (77.3%)\n",
      "2018-06-26T21:40:46.185-0400\t[########################] scrape.left\t485MB/485MB (100.0%)\n",
      "2018-06-26T21:40:46.185-0400\timported 1586 documents\n",
      "Collection left in scrape database created\n"
     ]
    }
   ],
   "source": [
    "# import json files into db\n",
    "left_path = 'db/json/by_media/left/' \n",
    "lefts = !ls {left_path}\n",
    "\n",
    "for file in lefts: \n",
    "    print(file)\n",
    "    COL_NAME='left'\n",
    "    addToDB(DB_NAME, COL_NAME, left_path, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NR_DailyB_Intercept.json\n",
      "2018-06-26T21:42:07.096-0400\tconnected to: localhost\n",
      "2018-06-26T21:42:10.083-0400\t[########................] scrape.right\t292MB/828MB (35.2%)\n",
      "2018-06-26T21:42:13.079-0400\t[###############.........] scrape.right\t548MB/828MB (66.2%)\n",
      "2018-06-26T21:42:16.079-0400\t[######################..] scrape.right\t774MB/828MB (93.5%)\n",
      "2018-06-26T21:42:16.807-0400\t[########################] scrape.right\t828MB/828MB (100.0%)\n",
      "2018-06-26T21:42:16.807-0400\timported 4768 documents\n",
      "Collection right in scrape database created\n",
      "bbc.json\n",
      "2018-06-26T21:42:17.022-0400\tconnected to: localhost\n",
      "2018-06-26T21:42:20.001-0400\t[####....................] scrape.right\t296MB/1.66GB (17.5%)\n",
      "^C\n",
      "2018-06-26T21:42:20.737-0400\tsignal 'interrupt' received; forcefully terminating\n",
      "Collection right in scrape database created\n",
      "guardian.json\n",
      "2018-06-26T21:42:20.914-0400\tconnected to: localhost\n"
     ]
    }
   ],
   "source": [
    "# import json files into db\n",
    "right_path = 'db/json/by_media/left/' \n",
    "rights = !ls {right_path}\n",
    "\n",
    "for file in rights: \n",
    "    print(file)\n",
    "    COL_NAME='right'\n",
    "    addToDB(DB_NAME, COL_NAME, right_path, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE = 'msnbc.json'\n",
    "\n",
    "addToDB(DB_NAME,COL_NAME,PATH,FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5b243fb2897e82028b8ffe50:\n",
      "has same meta\n",
      "----------\n",
      "5b243fb7897e82028b8ffe51:\n",
      "has same meta\n",
      "----------\n",
      "5b243fbc897e82028b8ffe52:\n",
      "has same meta\n",
      "----------\n",
      "5b243fc1897e82028b8ffe53:\n",
      "has same meta\n",
      "----------\n",
      "5b243fc6897e82028b8ffe54:\n",
      "has same meta\n",
      "----------\n",
      "5b243fd0897e82028b8ffe55:\n",
      "has same meta\n",
      "----------\n",
      "5b243fd5897e82028b8ffe56:\n",
      "has same meta\n",
      "----------\n",
      "5b243fda897e82028b8ffe57:\n",
      "has same meta\n",
      "----------\n",
      "5b243ff1897e82028b8ffe5b:\n",
      "has same meta\n",
      "----------\n",
      "5b243fec897e82028b8ffe5a:\n",
      "has same meta\n",
      "----------\n",
      "5b243fe0897e82028b8ffe58:\n",
      "has same meta\n",
      "----------\n",
      "5b243fe7897e82028b8ffe59:\n",
      "has same meta\n",
      "----------\n",
      "5b244001897e82028b8ffe5e:\n",
      "has same meta\n",
      "----------\n",
      "5b243ff6897e82028b8ffe5c:\n",
      "has same meta\n",
      "----------\n",
      "5b243ffc897e82028b8ffe5d:\n",
      "has same meta\n",
      "----------\n",
      "5b244007897e82028b8ffe5f:\n",
      "has same meta\n",
      "----------\n",
      "5b244011897e82028b8ffe61:\n",
      "has same meta\n",
      "----------\n",
      "5b24401b897e82028b8ffe63:\n",
      "has same meta\n",
      "----------\n",
      "5b24400c897e82028b8ffe60:\n",
      "has same meta\n",
      "----------\n",
      "5b244025897e82028b8ffe65:\n",
      "has same meta\n",
      "----------\n",
      "5b244016897e82028b8ffe62:\n",
      "has same meta\n",
      "----------\n",
      "5b244020897e82028b8ffe64:\n",
      "has same meta\n",
      "----------\n",
      "5b24402a897e82028b8ffe66:\n",
      "has same meta\n",
      "----------\n",
      "5b244031897e82028b8ffe67:\n",
      "has same meta\n",
      "----------\n",
      "5b244036897e82028b8ffe68:\n",
      "has same meta\n",
      "----------\n",
      "5b24403b897e82028b8ffe69:\n",
      "has same meta\n",
      "----------\n",
      "5b244040897e82028b8ffe6a:\n",
      "has same meta\n",
      "----------\n",
      "5b244050897e82028b8ffe6d:\n",
      "has same meta\n",
      "----------\n",
      "5b244045897e82028b8ffe6b:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-176-b71bb5be3dca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdocs_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtmlCol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-175-e130888c93ba>\u001b[0m in \u001b[0;36mdocs_parser\u001b[0;34m(htmlCol)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'html'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;31m# extract metadata from html\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mmeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'html'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-160-36e07114e768>\u001b[0m in \u001b[0;36mfeature_extractor\u001b[0;34m(html)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0marticle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnewspaper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArticle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0marticle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_html\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0marticle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# parse date manually if it wasn't found by newspaper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/newspaper/article.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    150\u001b[0m         \"\"\"\n\u001b[1;32m    151\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/newspaper/article.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0;31m# Before any computations on the body, clean DOM object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdocument_cleaner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtop_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_best_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/newspaper/cleaners.py\u001b[0m in \u001b[0;36mclean\u001b[0;34m(self, doc_to_clean)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mdoc_to_clean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove_nodes_regex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_to_clean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtwitter_re\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mdoc_to_clean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclean_para_spans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_to_clean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mdoc_to_clean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv_to_para\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_to_clean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'div'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0mdoc_to_clean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv_to_para\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_to_clean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'span'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdoc_to_clean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/newspaper/cleaners.py\u001b[0m in \u001b[0;36mdiv_to_para\u001b[0;34m(self, doc, dom_type)\u001b[0m\n\u001b[1;32m    225\u001b[0m                 'pre', 'table', 'ul']\n\u001b[1;32m    226\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdiv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdivs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m             \u001b[0mitems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetElementsByTags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdiv\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace_with_para\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/newspaper/parsers.py\u001b[0m in \u001b[0;36mgetElementsByTags\u001b[0;34m(cls, node, tags)\u001b[0m\n\u001b[1;32m    171\u001b[0m         selector = 'descendant::*[%s]' % (\n\u001b[1;32m    172\u001b[0m             ' or '.join('self::%s' % tag for tag in tags))\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0melems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0melems\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "docs_parser(htmlCol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_id : 5b243fbc897e82028b8ffe52\n",
      "url : https://www.msnbc.com/all-in/watch/white-house-can-t-fill-open-positions-turns-to-job-fair-125617056\n",
      "html : <!DOCTYPE html><html lang=\"en\" data-reactroot=\"\"><head><title data-rh=\"true\">White House can&#x27;t \n"
     ]
    }
   ],
   "source": [
    "show_doc('5b243fb2897e82028b8ffe50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
